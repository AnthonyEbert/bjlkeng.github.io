.. title: Probabilistic Interpretation of Regularization
.. slug: probabilistic-interpretation-of-regularization
.. date: 2016-06-25 15:52:33 UTC-04:00
.. tags: probability, regularization, Bayesian, mathjax
.. category: 
.. link: 
.. description: A look at regularization through the lens of probability.
.. type: text

.. |br| raw:: html

   <br />

.. |H2| raw:: html

   <br/><h3>

.. |H2e| raw:: html

   </h3>

.. |H3| raw:: html

   <h4>

.. |H3e| raw:: html

   </h4>

.. |center| raw:: html

   <center>

.. |centere| raw:: html

   </center>

This post is going to look at a probabilitic (Bayesian) interpretation of
regularization.  I'll take a look at regularization in the context of ordinary
linear regression where L1 (Lasso) and L2 (Ridge) regularization is quite
common.  We'll start off from the plain maximum likelihood estimate (MLE),
extend it to a maximum posterior estimate, and finally play around with the
priors to end up with L1 and L2 regularization.  Hopefully, this will help give
some intuition on why regularization is nothing more than specifying a prior on
the data.

.. TEASER_END

|h2| Regularization |h2e|

`Regularization <https://en.wikipedia.org/wiki/Regularization_(mathematics)>`_
is the process of introducing additional information in order to solve
ill-posed problems or prevent overfitting.  A trivial example is when trying to
fit a normal distribution but you only have one point.  In this case, you can't
estimate the variance (you need at least two points) so any MLE estimate which
*only* uses the data will be ill-formed.  Instead, by providing some
"additional information" you will be able to get a reasonable estimate for the
variance.

To make things a bit more concrete, let's talk about things in the context of a
`ordinary linear regression <https://en.wikipedia.org/wiki/Linear_regression>`_.
Recall from my previous post on 
`linear regression <link://slug/a-probabilistic-view-of-regression>`_ 
(Equation 11 in that post)
that the maximum likelihood estimate for ordinary is given by:

.. math::

    {\bf \hat{\beta}_{\text{MLE}}}
    &= \arg\min_{\bf \beta} \sum_{i=1}^{n} (y_i-\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p})^2 \\
    &= \arg\min_{\bf \beta} \sum_{i=1}^{n} (y_i-\hat{y_i})^2
    \tag{1}

The estimate is quite intuitive: pick the coefficients (:math:`\beta_i`) that
minimize squared error between the observed values (:math:`y_i`) and those
generated by our linear model (:math:`\hat{y_i}`).  

In the similar vein as above, consider what happens when we only have one data
point :math:`(y_0, {\bf x_0})` but more than on coefficient.  There are any
number of possible "lines" or equivalently coefficients that we could draw to
minimize Equation 1.  Thinking back the high school math this is analogous to 
estimating the slope and intercept for a line, we can't estimate it if we only
have a single point.  I'm using the example where we don't have enough data
but there could other types of issues such as 
`colinearity <https://en.wikipedia.org/wiki/Multicollinearity>`_ that may
not outright prevent fitting of the model but will probably produce an
unreasonable estimate.

Two common schemes for regularization add a simple change to Equation 1:

.. math::

    {\bf \hat{\beta}_{\text{L1}}}
    = \arg\min_{\bf \beta} \big( \sum_{i=1}^{n} (y_i-\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p})^2 
      + \lambda \sum_{i}^{n} | \beta | \big)
    \\
    \tag{2}

    {\bf \hat{\beta}_{\text{L2}}}
    = \arg\min_{\bf \beta} \big( \sum_{i=1}^{n} (y_i-\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p})^2 
      + \lambda \sum_{i}^{n} | \beta | ^2 \big)
    \tag{3}

`L1 regularization <https://en.wikipedia.org/wiki/Regularization_(mathematics)#Regularizers_for_sparsity>`_
(also known as LASSO in the context of linear regression) promotes sparsity of coefficients.
Sparsity translates to some coefficients having values, while others are zero
(or closer to zero).  This can be seen as a form of feature selection.
`L2 regularization
<https://en.wikipedia.org/wiki/Regularization_(mathematics)#Tikhonov_regularization>`_
(also known as ridge regression in the context of linear regression and
generally as Tikhonov regularization) promotes smaller coefficients (no one
coefficient should be too large).  This type of regulization is pretty common
and typically will help in producing reasonable estimates.  It also has a simple probabilistic
interpretation (at least in the context of linear regression) which we will see below.




|h2| The Likelihood Function |h2e|

Recall Equation 1 can be derived from the likelihood function (without
:math:`\log(\cdot)`) for ordinary linear regression:

.. math::

    \mathcal{L({\bf \beta}|{\bf y})} &:= P({\bf y} | {\bf \beta}) \\
        &= \prod_{i=1}^{n} P_Y(y_i|{\bf \beta}, \sigma^2) \\
        &= \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i-\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p})^2}{2\sigma^2}} 
    \tag{4}

where :math:`{\bf y}` are our observed data points (:math:`y_1, \ldots, y_N`) and
:math:`P_Y(y_i|\mu, \sigma^2)` is the probability of observing the data point :math:`y_i`.
The implicit assumption in linear regression is that the data points are normally 
distributed about the regression line (see my past post on
`linear regression <link://slug/a-probabilistic-view-of-regression>`_ for more on this).

Classical statistics typically focus on maximizing this likelihood function,
which *usually* provides a pretty good estimate -- except when it doesn't like
in the case of "small data".  However, looking at the problem from a more
probabilistic point of view (i.e. Bayesian), we don't just want to maximize
the likelihood function but rather the posterior probability.  Let's see how
that works.

|h2| The Posterior |h2e|

We'll start by reviewing 
`Bayes Theorem <https://en.wikipedia.org/wiki/Bayesian_inference#Formal>`_.
where we usually denote the parameters we're trying to estimate by
:math:`\theta` and the data as :math:`y`:

.. math::

    P(\theta | y) &= \frac{P(y | \theta) P(\theta)}{P(y)} \\
    \text{posterior} &= \frac{\text{likelihood} \cdot \text{prior}}{\text{evidence}}
    \tag{5}

In Bayesian inference, we're primarily concerned with the posterior: "the
probability of the parameters given the data".  Put in another way, we're looking
to estimate the probability distribution of the parameters (:math:`\theta`)
given the data we have observed (:math:`y`).  Contrast this with classical methods
which instead try to find the best parameters to maximize likelihood: the
probability of observing data (:math:`y`) given a different values of the
parameters.  Definitely a subtle difference but I think most would agree the Bayesian
interpretation is much more natural [1]_.

Looking at Equation 5 in more detail, we already know how to compute the likelihood but
the two new parts are the prior and the evidence.  This is where proponents of
frequentist statistics usually have a philosophical dilemma.  The prior is actually
something we (the modeller) explicitly choose that is **not** based on the data [2]_
(:math:`y`)  \*gasp\*!  

Without getting into a whole philosophical spiel, adding some additional prior
information is exactly what we want in certain situations!  For example when
we don't have enough data, we probably have some idea about what is reasonable
given our knowledge of the problem.  This prior allows us to encode this
knowledge.  Even in cases where we don't explicitly have this problem, we can
choose a `"weak" prior <https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors>`_
which will only bias the result slightly from the MLE estimate.  In cases where
we have lots of the data, the likelihood dominates Equation 5 anyways so the
result will be similar in these cases.

From Equation 5, a full Bayesian analysis would look at the distribution of the
parameters (:math:`\theta`).  However, most people will settle for something a
bit less involved: finding the maximum of the posterior.  This is known as the 
`maximum a posteriori probability estimate <https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation>`_, usually abreviated by MAP.  
This simlifies the analysis and in particular allows us to ignore the evidence
(:math:`P(y)`), which is constant relative to the parameters we're trying to estimate
(:math:`\theta`).

Formalizing the MAP estimate, we can write it as:

.. math::

    {\bf \hat{\theta}_{\text{MAP}}} &= \arg\max_{\bf \theta} P(\theta | y) \\
    &= \arg\max_{\bf \theta} \frac{P(y | \theta) P(\theta)}{P(y)} \\
    &= \arg\max_{\bf \theta} P(y | \theta) P(\theta) \\
    &= \arg\max_{\bf \theta} \log(P(y | \theta) P(\theta)) \\
    &= \arg\max_{\bf \theta} \log P(y | \theta) + \log P(\theta)
    \tag{6}

Notice that we can get rid of the evidence term (:math:`P(y)`) because it's
constant with respect to the maximization and also take the :math:`\log` of the
inner function because it's monotonically increasing.  Contrast this with the
MLE estimate:

.. math::

    {\bf \hat{\theta}_{\text{MLE}}} = \arg\max_{\bf \theta} \log P(y | \theta)
    \tag{7}

|h2| Selecting Priors for Linear Regression |h2e|

Finally, we are ready to understand how regularization works.

we can begin


|h2| Further Reading |h2e|

* Wikipedia: 
  `Regularization <https://en.wikipedia.org/wiki/Regularization_(mathematics)>`_,
  `ordinary linear regression <https://en.wikipedia.org/wiki/Linear_regression>`_,
  `Bayes Theorem <https://en.wikipedia.org/wiki/Bayesian_inference#Formal>`_,
  `"weak" prior <https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors>`_,
  `maximum a posteriori probability estimate <https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation>`_,
* A previous post on `linear regression <link://slug/a-probabilistic-view-of-regression>`_

|br|

.. [1] As you might have guessed, I'm fall into the Bayesian camp.  Although I would have to say that I'm much more of a pragmatist above all else.  I'll use whatever works, frequentist, Bayesian, no theoretical basis, doesn't really matter as long as I can solve the desired problem in a reasonable manner.  It just so happens Bayesian methods produce reasonable estimates very often.

.. [2] Well that's not exactly true.  As a modeller, we can pick a prior that does depend on the data, although that's a bit of "double dipping".  These methods are generally known as `empirical Bayes methods <https://en.wikipedia.org/wiki/Empirical_Bayes_method>`_.


