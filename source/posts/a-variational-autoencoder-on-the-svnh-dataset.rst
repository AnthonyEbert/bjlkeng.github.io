.. title: A Variational Autoencoder on the SVNH dataset
.. slug: a-variational-autoencoder-on-the-svnh-dataset
.. date: 2017-06-27 09:13:03 UTC-04:00
.. tags: variational calculus, svhn, autoencoders, Kullback-Leibler, generative models, mathjax
.. category: 
.. link: 
.. description: A writeup on using VAEs for the SVNH dataset.
.. type: text

.. |br| raw:: html

   <br />

.. |H2| raw:: html

   <br/><h3>

.. |H2e| raw:: html

   </h3>

.. |H3| raw:: html

   <h4>

.. |H3e| raw:: html

   </h4>

.. |center| raw:: html

   <center>

.. |centere| raw:: html

   </center>

In this post, I'm going to share some notes on implementing a variational
autoencoder (VAE) on the 
`Street View House Numbers <http://ufldl.stanford.edu/housenumbers/>`_ 
(SVHN) dataset.  My last post on 
`variational autoencoders <link://slug/variational-autoencoders>`__
showed a simple example on the MNIST dataset but because it was so simple I
thought I might have missed some of the subtler points of VAEs -- boy was I
right!  The fact that I'm not really computer vision guy nor a deep learning
guy didn't help either.  Through this exercise, I picked up some of the basics
in the "craft" of computer vision/deep learning area; there are a lot of subtle
points that are easy to gloss over if you're just reading someone else's
tutorial.  I'll share with you some of the details in the math (that I
initially got wrong) and also some of the implementation notes along with a
notebook that I used to train the VAE.  Please check out my previous post
on `variational autoencoders <link://slug/variational-autoencoders>`__,
which I'll assume you're read.

.. TEASER_END

|h2| The Street View House Numbers (SVHN) Dataset |h2e|

The `SVHN <http://ufldl.stanford.edu/housenumbers/>`__ is a real-world image
dataset with over 600,000 digits coming from natural scene images (i.e. Google
Street View images).  It has two formats: format 1 contains the full image with
meta information about the bounding boxes, while format 2 contains just the
cropped digits in 32x32 RGB format.  It has the same idea as the MNIST dataset
but much more difficult because the images are of varying styles, colour and
quality.

.. figure:: /images/svhn_format2.png
  :width: 400px
  :alt: SVHN Format 2 dataset
  :align: center

  Figure 1: SVNH format 2 cropped digits

Figure 1 shows a sample of the cropped digits from the SVHN website.  You can
see that you get a huge variety of digits, making the it much harder to train a
model.  What's interesting is that in some of the images, you have several
digits.  For example, "112" in the top row, is centered (and tagged) as "1" but
has additional digits that might cause problems when fitting a model.

|h2| A Quick Recap on VAEs |h2e|

Recall a variational autoencoder consists of two parts: a generative model
(the decoder network) and the approximate posterior (the encoder network)
with the latent variables sitting as the connection point between the two.

Writing the model out more formally, we have this for the generative model
which takes a latent variables :math:`Z` and generates a data sample
:math:`X`:

.. math::

    X &\sim \mathcal{N}(\mu_{X|z}, \sigma^2 * I) &&& \text{Observed variable} \\
    \mu_{X|z} &\sim g_{X|z}(Z; \theta) \\
    Z &\sim \mathcal{N}(0, I) 
    \tag{1}

To help fitting, we want to generate (an approximate) posterior distribution
for :math:`Z` given a :math:`X`:

.. math::

    z|X &\approx \mathcal{N}(\mu_{z|X}, \Sigma_{z|X}) \\
    \mu_{z|X}, \Sigma_{z|X} &= g_{z|X}(X; \phi) \\
    \tag{2}

Check out my `previous post <link://slug/variational-autoencoders>`__ on this
topic for more details and intuition.  Figure 1 shows an illustration of a
typical variational autoencoder.

.. figure:: /images/variational_autoencoder3.png
  :width: 450px
  :alt: Variational Autoencoder Diagram
  :align: center

  Figure 1: A typical variational autoencoder.

The key point for this discussion are the two objective functions (i.e. loss
functions).  The left hand side loss function is the KL divergence which
basically ensures that our encoder network is actually generating values that
match our standard isotropic Gaussian :math:`Z`.  The right hand side loss is
the log-likelihood of observing :math:`X` for our given output distribution
with parameters generated by our decoder network.  We want to minimize the sum
of both loss functions to ensure that our generative model works.  That is,
we can sample our :math:`Z` values from a standard isotropic Gaussian and our
network will faithfully generate sample that are close to the distribution of
:math:`X`.

|h2| Fixed or Fitted Variance? |h2e|

One subtlety that I glossed over in my previous post was the output variable
on :math:`X`.  Whenever I referenced the output variable, I assumed it was a
continuous normal distribution.  However, for the 
`MNIST example <https://github.com/bjlkeng/sandbox/blob/master/notebooks/variational-autoencoder.ipynb>`__
that I put together, the output variable was a Bernoulli.  Note that although
each :math:`X_i` is actually a discrete pixel value (say between 0 and 255), we
can still conveniently model it as a Bernoulli.  This is not exactly proper
since a Bernoulli only has 0/1 values but it works well in practice.

Let's take a look at what that looks like by taking the log-likelihood of the
output variable:

.. math::

    L(p | X) &= \log P(X|p)) \\
               &= \log \big[\prod_{i=1}^K p_i^{x_i}\big] \\
    &= \sum_{i=1}^K x_i \log p_i
    \tag{3}

which is just the `cross entropy
<https://en.wikipedia.org/wiki/Cross_entropy>`__ between :math:`X` and
:math:`p`, where :math:`K` is the length of :math:`X` and :math:`p`.  This
is precisely the output loss function that we used in the notebook.
Note that in this case :math:`X` is our observed value (i.e. MNIST image
that we train on) and :math:`p` is the value that our decoder network generates
(the analogue to :math:`\mu_{X|z}` above).

Let's try the same thing with a normal distribution as our output variable
(we'll treat each part of :math:`X` as independent since we're assuming a
diagonal covariance matrix):

.. math::

    L(\mu, \sigma^2 | X) &= \log P(X|\mu, \sigma^2)) \\
    &= \log \big[\prod_{i=1}^K \frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(x_i-\mu_i)^2}{2\sigma_i^2}} \big] \\
    &= -\frac{K}{2} \log(2\pi) 
       + \sum_{i=1}^K \big[ -\frac{\log \sigma_i^2}{2} 
                            -\frac{(x_i-\mu_i)^2}{2\sigma_i^2} \big]
    \tag{3}

Again, remember that :math:`X` values are our observed values (e.g. our image)
and :math:`\mu, \sigma^2` is what our fitted neural network is going to produce.

The big question here is what to do with :math:`\sigma^2`?  In Doersch's
tutorial (see references below), he sets :math:`\sigma_i^2` to a constant
hyperparameter that is the same across all dimensions of :math:`X` [1]_.  But
if you think about, why should the variance be the same across all dimensions?
For example, the top left corner of an image might always be a dark pixel so
its variance would be small.  In contrast, a pixel in the center might be very
different from image to image so it would have a high variance.  For many
cases, it doesn't quite make sense to set the variance to a constant
hyperparameter across all pixels.  

This was initially a big roadblock for me because I was primarily following
Doersch's tutorial.  However, one of the original VAE papers by Kingma (see
references below) actually explicitly models each :math:`\sigma_i^2` separately
as another output of their decoder network.  In one Kingma's later papers, he
uses a VAE for comparison on the SVHN dataset and even has an `implementation
<https://github.com/dpkingma/nips14-ssl/>`__!  Had I just looked at it sooner,
it would have been much easier.  However, this wasn't the only misunderstanding
that I had.

|h3| Modelling :math:`\sigma^2` |h3e|

A couple of notes on implementing a loss function with :math:`\sigma^2`.
I had a lot of trouble implementing Equation 3 as Kingma did in his paper.  His
decoder network outputs two values: the mean :math:`\mu_i` and log-variance
:math:`u:=\log\sigma^2`.  He uses a standard dense fully connected layer for both
with no activiation function.  This allows both values to range from
:math:`(-\infty, \infty)`.  However, notice how Equation 3 changes with this change 
of variables:

.. math::

    L(\mu, \sigma^2 | X) &= \log P(X|\mu, \sigma^2)) \\
    &= -\frac{K}{2} \log(2\pi) 
       + \sum_{i=1}^K \big[ -\frac{\log \sigma_i^2}{2} 
                            -\frac{(x_i-\mu_i)^2}{2\sigma_i^2} \big] \\
    &= -\frac{K}{2} \log(2\pi) 
       + \sum_{i=1}^K \big[ -\frac{u_i}{2} 
                            -\frac{(x_i-\mu_i)^2}{2exp(u_i)} \big]
    \tag{3}

You're now dividing by an exponential in the last term.  If :math:`u` gets
anywhere close to :math:`0`, your loss function blows up.  I've seen more
"nan"'s in this project than all my other work with other types of models :p

One solution to this problem I found in a 
`Reddit thread <https://www.reddit.com/r/MachineLearning/comments/4eqifs/gaussian_observation_vae/>`__ 
(after much trial and error).  Basically, we can model :math:`\sigma^2` directly
plus an epsilon to ensure that it doesn't blow up. So we define the output of
our network as :math:`v:=\sigma^2 + \epsilon` and use a `"ReLU"
<https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>`__ activation
function to ensure that we only get positive numbers.  Let's take a look
at this new change of variables:

.. math::

    L(\mu, \sigma^2 | X) &= \log P(X|\mu, \sigma^2)) \\
    &= -\frac{K}{2} \log(2\pi) 
       + \sum_{i=1}^K \big[ -\frac{\log \sigma_i^2}{2} 
                            -\frac{(x_i-\mu_i)^2}{2\sigma_i^2} \big] \\
    &= -\frac{K}{2} \log(2\pi) 
       + \sum_{i=1}^K \big[ -\frac{\log(v + \epsilon)}{2} 
                            -\frac{(x_i-\mu_i)^2}{2(v + \epsilon)} \big]
    \tag{4}


:math:`\epsilon` is a hyperparameter that needs to be small enough to be able
to capture the variance of your smallest dimension (bigger variances can be
generated by the "ReLU" function).  Along with this issue, you have to make
sure that you're learning rate is small enough to allow your loss function to
converge because we're still dividing by potentially small values that can vary
widely [2]_. 

|h2| A Digression on Progress |h2e|

Pace is moving so fast that I can state-of-the-art research from just a few
years ago in my spare time.

|h2| Convolution or Components? |h2e|

Explain why it's hard with CNNs

* PCA (whitening didn't work)
* cnns didn't work
  * Point to reddit post

|h2| Implementation Notes |h2e|

Annotated notebook, but here are some of the (obvious to most people) points
that I figured

* batch norm, "RELU", dropout
* bigger batch size to speed up iterations (if gpu is big enough)
* epoch = 1000
  * Show graph of loss
* use entire dataset with keras fit_generator
* big network with regularization (link to CS291 course)
* Explain intuition about loss function with normal distribution + PCA, optimize big term first, and then go downward
* Keras backend functions operate on the entire tensor, need to use "axis=-1" to operate on non-batch_size dims
* Lower learning rate to 0.0001 (from default 0.001, 0.005)
  * (Sometimes) Blow up gradient otherwise

|h2| VAE SVNH Results |h2e|

* Fun part: generate images
* Randomly generated numbers
* Analogies 


|h2| Further Reading |h2e|

* Previous Posts: `variational autoencoders <link://slug/variational-autoencoders>`__
* Wikipedia: `Cross Entropy <https://en.wikipedia.org/wiki/Cross_entropy>`__
* Relevant Reddit Posts:  `Gaussian observation VAE <https://www.reddit.com/r/MachineLearning/comments/4eqifs/gaussian_observation_vae/>`__, `Results on SVHN with Vanilla VAE? <https://www.reddit.com/r/MachineLearning/comments/3wp5pc/results_on_svhn_with_vanilla_vae/>`__
* "Tutorial on Variational Autoencoders", Carl Doersch, https://arxiv.org/abs/1606.05908
* "Auto-Encoding Variational Bayes", Diederik P. Kingma, Max Welling, https://arxiv.org/abs/1312.6114 
* "Code for reproducing results of NIPS 2014 paper "Semi-Supervised Learning with Deep Generative Models", Diederik P. Kingma, https://github.com/dpkingma/nips14-ssl/
* The Street View House Numbers (SVHN) Dataset, http://ufldl.stanford.edu/housenumbers/
* CS231n: Convolutional Neural Networks for Visual Recognition, Stanford University, http://cs231n.github.io/neural-networks-1/
* PCA Whitening - UFLDL Tutorial, Stanford University, http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/
  
|br|


.. [1] I'm sure Doersch knows that this is not always the right thing to do.  He probably was just trying to simplify the explanation in the tutorial to make the flow smoother.

.. [2] I'm sure the learing rate and initialization of weights is somehow related to how Kingma's implementation works.  I didn't really spend much time trying to figure this out after I got my implementatin working though.
