{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One of the most common probability distributions is the normal (or Gaussian) distribution.  Many natural phenomena can be modelled using a normal distribution.  It's also of great importance due to its relation to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).\n",
    "\n",
    "In this post, we'll be reviewing the normal distribution and looking at how to draw samples from it using two methods.  One naive method using the central limit theorem, and another using the [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform).  As usual, some brief coverage of proofs and code will be included to help drive intuition.\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "### Normal Distribution\n",
    "\n",
    "Let's first start off covering some basics.  A normal distribution (also known as a Gaussian distribution) \\\\(N \\sim (\\mu, \\sigma)\\\\) has probability density function (PMF) and cumulative density function (CDF) shown here [<sup>[1]</sup>](#fn-1):\n",
    "\n",
    "$$ f_N(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma}} \\tag{1}$$\n",
    "$$ F_N(x) = \\int_{-\\infty}^{x}f_N(t) dt  \\tag{2}$$\n",
    "\n",
    "The CDF doesn't have a nice closed form, so we'll just represent it here using the definition of CDF in terms of its PDF.  We can graph the PDF and CDF (images from [Wikipedia](https://en.wikipedia.org/wiki/Normal_distribution)):\n",
    "\n",
    "![PDF of Normal Distribution](/images/normal_pdf_cdf.png)\n",
    "\n",
    "The normal distribution is sometimes colloquially known as the \"bell curve\" because of a it's symmetric hump.  A very common thing with a probability distribution is to *sample* from it.  In other words, we want to randomly generate numbers (i.e. \\\\(x\\\\) values) such that the values of \\\\(x\\\\) are in proportion to the PDF.  So for the standard normal distribution, \\\\(N \\sim (0, 1) \\\\) (the red curve in the picture above), most of the values would fall close to somewhere around \\\\(x=0\\\\).  In fact, 68% will fall within \\\\([-1, 1]\\\\), 95% will fall within \\\\([-2,2]\\\\) and 99.7% will fall within \\\\([-3,3]\\\\).  This corresponds to \\\\(\\sigma, 2\\sigma, 3\\sigma\\\\) from the mean, see this [article](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) for more details.\n",
    "\n",
    "### Central Limit Theorem\n",
    "\n",
    "The central limit theorem (CLT) is quite a surprising result relating the sample average of \\\\(n\\\\) [indepededent and identially distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d.) random variables and a normal distribution.  To state more precisely: \n",
    "\n",
    "> Let \\\\({X_1, X_2, \\ldots, X_n}\\\\) be \\\\(n\\\\) i.i.d. random variables with \\\\(E(X_i)=\\mu\\\\) \n",
    "> and \\\\(Var(X_i)=\\sigma^2\\\\) and  let \\\\(S_n = \\frac{X_1 + X_2 + \\ldots + X_n}{n}\\\\) be the sample \n",
    "> average. Then \\\\(S_n\\\\) approximates a normal distribution with mean of \\\\(\\mu\\\\) and \n",
    "> variance of \\\\(\\frac{\\sigma^2}{n}\\\\) for large \\\\(n\\\\) (i.e. \\\\(S_n \\approx N(\\mu, \\frac{\\sigma^2}{n})\\\\))\n",
    "\n",
    "The surprising result is that \\\\(X_n\\\\) can be *any* shape of distribution.  It isn't restricted to just normal distributions.  We can also define the standard normal distribution in terms of \\\\(S_n\\\\) by shifting and scaling it:\n",
    "\n",
    "$$ N(0,1) \\approx \\frac{S_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\sqrt{n}(S_n - \\mu)}{\\sigma} \\tag{3} $$\n",
    "\n",
    "### Comparing Distributions\n",
    "\n",
    "Since our goal is to implement sampling from a normal distribution, it would be nice to know if we actually did it correctly!  One common way to test if two arbitrary distributions are the same is to use the [Kolmogorov–Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).  In the basic form, we can compare a sample of points with a reference distribution to find their similarity.  \n",
    "\n",
    "The basic idea of the test is to sort the points in the sample and the compute the [empirical CDF](https://en.wikipedia.org/wiki/Empirical_distribution_function).  \n",
    "Then compare the absolute difference between any point in the ECDF and the theoretical reference distribution.  If the two are the same, this difference should be very small. If it's large then we can be confident that the distribution is different.  Further, this difference follows a certain distribution, which allows us to test our null hypothesis of whether our samples were drawn from the reference distribution.  \n",
    "The following figure (from [Wikipedia](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution)) shows this more clearly:\n",
    "\n",
    "![Kolmogorov–Smirnov test](/images/ks_test.png)\n",
    "\n",
    "Fortunately, we don't have to implement this ourselves.  A package is available in [scipy.stats](http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html).  Let's play around with it a bit to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U(0,1) vs. N(0, 1): KS=0.5000 with p-value = 0.0000.\n",
      "N(0,2) vs. N(0, 1): KS=0.1719 with p-value = 0.0000.\n",
      "N(0,1) vs. N(0, 1): KS=0.0131 with p-value = 0.3564.\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123) \n",
    "\n",
    "# number of samples\n",
    "N=5000\n",
    "\n",
    "# Run Kolmogorov-Smirnov test Uniform(0,1) vs. reference N(0, 1)\n",
    "samples = stats.uniform.rvs(loc=0, scale=1, size=N)\n",
    "test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n",
    "print(\"U(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))\n",
    "\n",
    "# Run Kolmogorov-Smirnov test N(0, 2) vs. reference N(0, 1)\n",
    "samples = stats.norm.rvs(loc=0, scale=2, size=N) \n",
    "test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n",
    "print(\"N(0,2) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))\n",
    "\n",
    "# Run Kolmogorov-Smirnov test N(0, 1) vs. reference N(0, 1)\n",
    "samples = stats.norm.rvs(loc=0, scale=1, size=N) \n",
    "test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n",
    "print(\"N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \\\\(N(0,1)\\\\) as our reference distribution, the KS test has a large value and a neglible p-value when comparing to a uniform distribution \\\\(U(0,1)\\\\) (\\\\(KS=(0.5\\\\)) as well a normal distribution with a wider base \\\\(N(0, 2)\\\\) (\\\\(KS=0.1719\\\\)).  However, when we compare samples from the identical distribution \\\\(N(0,1)\\\\), we get a relatively small value (\\\\(KS=0.0131\\\\)) for the test statistic and a large p-value, indicating we have no evidence to reject the null hypothesis (that our two distributions are the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Sampling using the Central Limit Theorem\n",
    "\n",
    "Now let's try to use the Central Limit Theorem to sample from \\\\(N(0,1)\\\\).  First let's define our i.i.d. variable \\\\(X_n\\\\) to have [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with \\\\(p=0.5\\\\), which we can intuitively think of tossing an unbiased coin:\n",
    "\n",
    "$$ P(X_n=k) = \\begin{cases} p=0.5 & \\text{if }k=1, \\\\[6pt] 1-p = 0.5 & \\text {if }k=0.\\end{cases}</math> \\tag{4} $$\n",
    "\n",
    "Recall, the Bernoulli distribution is closely relates to the [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) denoted by \\\\(B(n, p)\\\\) by \\\\Bernoulli(p) = B(n=1, p\\\\) .  The Binomial distribution can intuitively be thought of as counting the number of heads in \\\\(n\\\\) tosses of a coin (i.e. Bernoulli trials).  If \\\\(n=1\\\\), it reduces to a single Bernoulli distribution (or coin toss).\n",
    "\n",
    "Let's now define our sample average for \\\\(n\\\\) tosses of our unbiased coin:\n",
    "\n",
    "$$ S_n = \\frac{X_1 + X_2 + \\ldots + X_n}{n} = \\frac{B(n, p=0.5)}{n} $$\n",
    "\n",
    "We can see that this distribution has \\\\(mu=\\frac{n}{2}\\\\) (we expect half our tosses to be heads), and \\\\(\\sigma^2=\\frac{p(1-p)}{n}=\\frac{0.25}{n}\\\\) (Bernoulli RVs have \\\\(\\sigma^2 = p(1-p)\\\\)).\n",
    "\n",
    "Shifting and scaling[<sup>[2]</sup>](#fn-2) this to get our standard normal distribution using Equation 3, we get:\n",
    "\n",
    "\\begin{align} \n",
    "N(0,1) &\\approx \\frac{\\sqrt{n}(S_n - \\mu)}{\\sigma} \\\\\n",
    "       &= \\frac{\\sqrt{n}(\\frac{X_1 + X_2 + \\ldots + X_n}{n} - 0.5)}{\\sqrt{0.25}} \\\\\n",
    "       &= 2\\sqrt{n}(\\frac{X_1 + X_2 + \\ldots + X_n}{n} - 0.5) \\tag{5}\n",
    "\\end{align} \n",
    "\n",
    "Theoretically, this should give us an equation to roughly simulate a standard normal distribution.  Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_N(0,1) vs. N(0, 1): KS=0.0162 with p-value = 0.1426.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f62c611cdd8>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxCAiIlmUGEREJIsSg4iIZFFiEBGRLP8DOMkrHM6j9zwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62c6461da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "N=5000\n",
    "random.seed(123)\n",
    "\n",
    "def bernoulli():\n",
    "    return random.randint(0,1)\n",
    "\n",
    "# Our sample function of N(0,1) using Equation 5\n",
    "def sampleN_v1(N=5000):\n",
    "    return 2.0 * math.sqrt(N) * (sum(bernoulli() for x in range(N)) / N - 0.5)\n",
    "\n",
    "# Use KS to test again\n",
    "samples = [sampleN_v1() for x in range(N)]\n",
    "test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n",
    "print(\"sample_N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))\n",
    "\n",
    "# Let's plot our samples against our reference distribution\n",
    "reference = [stats.norm.rvs() for x in range(N)]\n",
    "pd.DataFrame({'sample_N': samples, 'N(0,1)': reference}).plot(kind='hist', bins=100, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our KS score is somewhat close to \\\\(0\\\\) with a p-value that suggests we can't reject our null hypothesis.  Graphing the our implementation of \\\\(N(0,1)\\\\) with the reference one shows that we have the roughly the right shape.  No doubt by setting larger \\\\(N\\\\) (for both the number of Bernoulli trials and the number of samples drawn), we would get something much closer.  \n",
    "\n",
    "One large downside of our CLT implementation is that it's *slow*.  While the wall clock time of drawing \\\\(5000\\\\) samples using the `numpy` library is unnoticeable, it takes roughly ten seconds on my machine with the parameters above, quite a big difference.  In the next section, we'll see a much more efficient implementation that uses a \"trick\" to transform a pair of independent uniform random variables to a pair of independent normal random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the Box-Muller Transform\n",
    "\n",
    "The [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is a neat little \"trick\" that allows us to sample from a pair of normally distributed variables using only a source of uniformly distributed variables.  The transform is actually pretty simple to compute.  Given two independent uniformally distributed random variables \\\\(U_1, U_2\\\\) on the interval \\\\((0,1)\\\\), we define two new random variables \\\\(R and \\Theta\\\\) intuitively representing polar coordinates as such:\n",
    "\n",
    "\\begin{align}\n",
    "R    &= \\sqrt{-2lnU_1}   \\tag{6}\\\\\n",
    "\\Theta &= 2\\pi U_2  \\tag{7}\n",
    "\\end{align}\n",
    "\n",
    "Now using the standard transformation from polar coodinates \\((R, \\Theta\\\\) to cartesian ones \\\\(X, Y\\\\), we claim that \\\\(X\\\\) and \\\\(Y\\\\) are independent standard normally distributed random variables:\n",
    "\n",
    "\\begin{align}\n",
    "X &= Rcos\\Theta &= \\sqrt{-2lnU_1}cos(2\\pi U_2) \\tag{8}\\\\\n",
    "Y &= Rsin\\Theta &= \\sqrt{-2lnU_1}sin(2\\pi U_2) \\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "Let's sketch the proof to gain some intuition on how this works.\n",
    "\n",
    "### Proof\n",
    "\n",
    "Starting with \\\\(U_1, U_2\\\\), let's see what kind of distributions we have for \\\\(R, \\Theta\\\\).  From Equation 7, it should be clear that \\\\(\\Theta\\\\) is also uniformly distributed since it's just multiplying by a constant (\\\\(2\\pi\\\\)), but let's go through the motions to explicitly see that.  From the CDF of \\\\(\\Theta\\\\):\n",
    "\n",
    "\\begin{align}\n",
    "P(\\Theta \\leq \\theta) &= P(2\\pi U_2 \\leq \\theta) \\\\\n",
    "                      &= P(U_2 \\leq \\frac{\\theta}{2\\pi}) \\\\\n",
    "               \\Theta &\\sim Uniform(0, 2\\pi) \\tag{10}\n",
    "\\end{align}\n",
    "\n",
    "So we have our first result that \\\\(\\Theta\\\\) is uniformally distributed on \\\\((0, 2\\pi)\\\\) (as we would expect).  Using a more explicit method, we can find our the distribution of \\\\(R\\\\).  From Equation 7, we know that the mapping from \\\\(U_1\\\\) to \\\\(R\\\\) is one-to-one, which we'll call \\\\(g\\\\) (i.e. \\\\(R=g(U_1)\\\\)).  So if we're trying to find the probability of R over \\\\(r_1, r_2\\\\) (by integrating its PDF), there is some equivalent range in U_1 over \\\\(g^{-1}(r_1), g^{-1}(r_2)\\\\) where we can integreate over the PDF of \\\\(U_1\\\\).  Let's see how this works [<sup>[3]</sup>](#fn-3):\n",
    "\n",
    "\\begin{align}\n",
    "P(r_1 \\leq R \\leq r_2) &=  \\int_{g^{-1}(r_2)}^{g^{-1}(r_1)} f_{U_1}(s) ds  && \\text{since } g(x) \\text{ is strictly decreasing}\\\\\n",
    "                       &=  -\\int_{g^{-1}(r_1)}^{g^{-1}(r_2)}  ds && \\text{since }f_{U_1}(u)=1 \\\\\n",
    "                       &=  -\\int_{r_1}^{r_2} -t e^{\\frac{-t^2}{2}} dt && \\text{change variables } t=\\sqrt{-2\\ln s}\\tag{11} \\\\\n",
    "                f_R(r) &=  r e^{\\frac{-r^2}{2}}  \\tag{12}\n",
    "\\end{align}\n",
    "\n",
    "giving us the PDF for \\\\(R\\\\), \\\\(f_R(r)\\\\).  It should also be clear that \\\\(R\\\\) and \\\\(\\Theta\\\\) are independent because \\\\(U_1\\\\) and \\\\(U_2\\\\) are independent.\n",
    "\n",
    "Our result is actually a specific case of a more general result when transforming from one probability distribution, \\\\(A\\\\), to another, \\\\(B\\\\), using a one-to-one mapping, \\\\(B = g(A)\\\\):\n",
    "\n",
    "\\begin{align}\n",
    "    f_{B}(b) &= f_{A}({g^{-1}(b)})|\\frac{d g^{-1}(b)}{d b}| \\\\\n",
    "             &= f_{A}({a})|\\frac{d a}{d b}|  && \\text{where } a = g^{-1}(b)  \\tag{13}\n",
    "\\end{align}\n",
    "\n",
    "If you look at Equation 13, you'll see that the derivative is implicitly what we're doing when we change variables in the integral in Equation 11.  Take a look at [Transformations of Random Variables](http://www.math.uah.edu/stat/dist/Transformations.html) for more details and examples.\n",
    "\n",
    "Now that we have the distributions for both \\\\(R\\\\) and \\\\(\\Theta\\\\), let's label Equation 8 and 9 as \\\\(X=g_x(R, \\Theta)\\\\) and \\\\(Y=g_y(R, \\Theta)\\\\), respectively.  Now we can apply the same procedure as above using variable substitution (for multiple variables) to derive the joint distribution of \\\\(X\\\\) and \\\\(Y\\\\) :\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "P(x_1 &\\leq X \\leq x_2, y_1 \\leq Y \\leq y_2) \\\\\n",
    "    &= \\int_{g_x^{-1}(x_1)}^{g_x^{-1}(x_2)} \\int_{g_y^{-1}(y_1)}^{g_y^{-1}(y_2)} f_{R,\\Theta}(u, v) du dv \\\\\n",
    "    &= \\int_{g_x^{-1}(x_1)}^{g_x^{-1}(x_2)} \\int_{g_y^{-1}(y_1)}^{g_y^{-1}(y_2)} f_{R}(u)f_{\\Theta}(v) du dv && \\text{since } R \\text{ and } \\Theta \\text{ are independent} \\\\ \n",
    "    &= \\int_{g_x^{-1}(x_1)}^{g_x^{-1}(x_2)} \\int_{g_y^{-1}(y_1)}^{g_y^{-1}(y_2)} ue^{\\frac{-u^2}{2}}\\frac{1}{2\\pi} du dv \\tag{14}\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "At this point, we should remember that \\\\(p=ucos(v), q=usin(v)\\\\), so solving for \\\\(u,v\\\\) results in \\\\(u=\\sqrt{q^2+p^2}, v=arctan(\\frac{q}{p})\\\\).  Also, [substition for multiple variables](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables) means that \\\\(du dv = |det(\\frac{\\partial(u,v)}{\\partial(p, q)})| dp dq\\\\), plugging our expressions for \\\\(p\\\\) and \\\\(q\\\\) in:\n",
    "\n",
    "\\begin{align}\n",
    "du dv &= |det(\\frac{\\partial(p,q)}{\\partial(u, v)})| dp dq \\\\\n",
    "      &= \n",
    "\\left |\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial u}{\\partial p} & \\frac{\\partial u}{\\partial q} \\\\\n",
    "\\frac{\\partial v}{\\partial p} & \\frac{\\partial v}{\\partial q}\n",
    "\\end{array}\n",
    "\\right | dpdq \\\\\n",
    "      &= \n",
    "\\left |\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial \\sqrt{q^2+p^2}}{\\partial p} & \\frac{\\partial \\sqrt{q^2+p^2}}{\\partial q} \\\\\n",
    "\\frac{\\partial arctan(\\frac{q}{p})}{\\partial p} & \\frac{\\partial arctan(\\frac{q}{p})}{\\partial q}\n",
    "\\end{array}\n",
    "\\right | dpdq \\\\\n",
    "      &= \n",
    "\\left |\n",
    "\\begin{array}{cc}\n",
    "\\frac{p}{\\sqrt{p^2+q^2}} & \\frac{q}{\\sqrt{p^2+q^2}} \\\\\n",
    "\\frac{-q}{p^2(1+(\\frac{q}{p})^2)} & \\frac{1}{p(1+(\\frac{q}{p})^2)}\n",
    "\\end{array}\n",
    "\\right | dpdq \\\\\n",
    "      &= \\frac{1}{\\sqrt{p^2 + q^2}} dpdq \\tag{15}\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "Using the result from Equation 15 into Equation 14, we get:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "P(x_1 &\\leq X \\leq x_2, y_1 \\leq Y \\leq y_2) \\\\\n",
    "    &= \\int_{x_1}^{x_2} \\int_{y_1}^{y_2} \\frac{\\sqrt{p^2 + q^2}}{2\\pi}\\cdot e^{\\frac{-(p^2 + q^2)}{2}} \\cdot \\frac{1}{\\sqrt{p^2 + q^2}} dp dq  && \\text{change variables} \\\\\n",
    "    &                                                                             && p=ucos(v), q=usin(v) \\\\\n",
    "    &= \\int_{x_1}^{x_2} \\underbrace{\\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-p^2}{2}}}_{f_X(x)} dp \\int_{y_1}^{y_2} \\underbrace{\\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-q^2}{2}}}_{f_Y(y)} dq  \\tag{16}\n",
    "\\end{align}\n",
    "\n",
    "Equation 16 shows that \\\\(X\\\\) and \\\\(Y\\\\) are independent each with PDF matching our standard normal distribution \\\\(N(0,1)\\\\).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "\n",
    "* Wikipedia: [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution), [Box-Muller Transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), [Substition for multiple variables](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables), [Determinant of 2x2 matrices](https://en.wikipedia.org/wiki/Determinant#2.C2.A0.C3.97.C2.A02_matrices), [Jacobian matrix and determinant](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)\n",
    "* Stack Exchange: [How to sample from a normal distribution with known mean and variance using a conventional programming language?](http://stats.stackexchange.com/questions/16334/how-to-sample-from-a-normal-distribution-with-known-mean-and-variance-using-a-co), [Proof of Box-Muller method](http://math.stackexchange.com/questions/1110168/proof-of-the-box-muller-method)\n",
    "* [Transformations of Random Variables](http://www.math.uah.edu/stat/dist/Transformations.html) (University of Alabama Huntsville)\n",
    "* [Change of Variables](http://tutorial.math.lamar.edu/Classes/CalcIII/ChangeOfVariables.aspx) (Paul's Online Math Notes)\n",
    "* [Simple Sampling of Gaussians](http://www.math.nyu.edu/faculty/goodman/teaching/MonteCarlo2005/notes/GaussianSampling.pdf) (Jonathan Goodman, NYU)\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "List of Notes: [^1], [^2], [^3]\n",
    "\n",
    "[^1]: We'll use the convention of \\\\(f_X(x)\\\\) and \\\\(F_X(x)\\\\) to denote the PDF and CDF of random variable X, respectively.\n",
    "\n",
    "[^2]: If we got back to the definitions of mean and variance, we can see this shifting and scaling yeilds the correct result.  \\\\(E(\\frac{\\sqrt{n}(S_n - \\mu)}{\\sigma}) = \\frac{\\sqrt{n}(E(S_n) - \\mu)}{\\sigma} = 0\\\\) since \\\\(E(S_n)=\\mu\\\\) by definition.  And \\\\(Var(\\frac{\\sqrt{n}(S_n - \\mu)}{\\sigma}) = \\frac{n}{\\sigma^2}(E((S_n - \\mu)^2) - (E(S_n - \\mu))^2 = \\frac{n}{\\sigma^2} \\frac{\\sigma^2}{n} = 1 \\\\)\n",
    "\n",
    "[^3]: We use \\\\(s, t\\\\) for the dummy variables of \\\\(U_1, R\\\\) respectively.  You might also need a refresher on [integration by substitution](https://en.wikipedia.org/wiki/Integration_by_substitution) like I did.  Also notice that we flipped the integral endpoints of \\\\(g^{-1}(r1)\\\\) and \\\\(g^{-1}(r2)\\\\) because \\\\(g(x)\\\\) is a strictly decreasing function, so the lower limit \\\\(r_1\\\\) maps to the upper limit \\\\(g^{-1}(r_1)\\\\) in the transformed function.  Similarly with \\\\(r_2\\\\) and \\\\(g^{-1}(r_2)\\\\).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
