<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Bounded Rationality (probabilitiy)</title><link>http://satisficing.briankeng.com/</link><description></description><atom:link rel="self" type="application/rss+xml" href="http://satisficing.briankeng.com/categories/probabilitiy.xml"></atom:link><language>en</language><lastBuildDate>Fri, 27 Jan 2017 13:53:20 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Maximum Entropy Distributions</title><link>http://satisficing.briankeng.com/posts/maximum-entropy-distributions/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post will talk about a method to find the probability distribution that best
fits your given state of knowledge about some data.  Using the principle of maximum
entropy and some testable information (e.g. the mean), you can find the
distribution that makes the fewest assumptions (the one with maximal
information entropy).  As you may have guessed, this is used often in Bayesian
inference to determine prior distributions and also (at least implicitly) in
natural language processing applications with maximum entropy (MaxEnt)
classifiers (i.e. a multinomial logistic regression).  As usual, I'll go through
some intuition, some math, and some examples.  Hope you find this topic as
interesting as I do!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/maximum-entropy-distributions/"&gt;Read moreâ€¦&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>entropy</category><category>mathjax</category><category>probabilitiy</category><guid>http://satisficing.briankeng.com/posts/maximum-entropy-distributions/</guid><pubDate>Fri, 06 Jan 2017 13:05:00 GMT</pubDate></item></channel></rss>