<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bounded Rationality (mathjax)</title><link>http://satisficing.briankeng.com/</link><description></description><atom:link rel="self" type="application/rss+xml" href="http://satisficing.briankeng.com/categories/mathjax.xml"></atom:link><language>en</language><lastBuildDate>Tue, 13 Dec 2016 13:48:52 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Lagrange Multipliers</title><link>http://satisficing.briankeng.com/posts/lagrange-multipliers/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to be about finding the maxima or minima of a function
subject to some constraints.  This is usually introduced in a multivariate
calculus course, unfortunately (or fortunately?) I never got the chance to take
a multivariate calculus course that covered this topic.  In my undergraduate class, computer
engineers only took three half year engineering calculus courses, and the
&lt;a class="reference external" href="http://www.ucalendar.uwaterloo.ca/1617/COURSE/course-ECE.html#ECE206"&gt;fourth one&lt;/a&gt;
(for electrical engineers) seems to have covered other basic multivariate
calculus topics such as all the various theorems such as Green's, Gauss', Stokes' (I
could be wrong though, I never did take that course!).  You know what I always imagined Newton
saying, "It's never too late to learn multivariate calculus!".&lt;/p&gt;
&lt;p&gt;In that vein, this post will discuss one widely used method for finding optima
subject to constraints: Lagrange multipliers.  The concepts
behind it are actually quite intuitive once we come up with the right analogue
in physical reality, so as usual we'll start there.  We'll work through some
problems and hopefully by the end of this post, this topic won't seem as
mysterious anymore &lt;a class="footnote-reference" href="http://satisficing.briankeng.com/posts/lagrange-multipliers/#id3" id="id1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/lagrange-multipliers/"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>calculus</category><category>lagrange multipliers</category><category>mathjax</category><guid>http://satisficing.briankeng.com/posts/lagrange-multipliers/</guid><pubDate>Tue, 13 Dec 2016 12:48:31 GMT</pubDate></item><item><title>The Expectation-Maximization Algorithm</title><link>http://satisficing.briankeng.com/posts/the-expectation-maximization-algorithm/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to talk about a widely used method to find the
maximum likelihood (MLE) or maximum a posteriori (MAP) estimate of parameters
in latent variable models called the Expectation-Maximization algorithm.  You
have probably heard about the most famous variant of this algorithm called the
k-means algorithm for clustering.
Even though it's so ubiquitous, whenever I've tried to understand &lt;em&gt;why&lt;/em&gt; this
algorithm works, I never quite got the intuition right.  Now that I've taken
the time to work through the math, I'm going to &lt;em&gt;attempt&lt;/em&gt; to explain the
algorithm hopefully with a bit more clarity.  We'll start by going back to the
basics with latent variable models and the likelihood functions, then moving on
to showing the math with a simple Gaussian mixture model &lt;a class="footnote-reference" href="http://satisficing.briankeng.com/posts/the-expectation-maximization-algorithm/#id5" id="id1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/the-expectation-maximization-algorithm/"&gt;Read more…&lt;/a&gt; (18 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>expectation-maximization</category><category>gaussian mixture models</category><category>latent variables</category><category>mathjax</category><guid>http://satisficing.briankeng.com/posts/the-expectation-maximization-algorithm/</guid><pubDate>Fri, 07 Oct 2016 12:47:47 GMT</pubDate></item><item><title>A Probabilistic Interpretation of Regularization</title><link>http://satisficing.briankeng.com/posts/probabilistic-interpretation-of-regularization/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to look at a probabilistic (Bayesian) interpretation of
regularization.  We'll take a look at both L1 and L2 regularization in the
context of ordinary linear regression.  The discussion will start off
with a quick introduction to regularization, followed by a back-to-basics
explanation starting with the maximum likelihood estimate (MLE), then on to the
maximum a posteriori estimate (MAP), and finally playing around with priors to
end up with L1 and L2 regularization.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/probabilistic-interpretation-of-regularization/"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Bayesian</category><category>mathjax</category><category>probability</category><category>regularization</category><guid>http://satisficing.briankeng.com/posts/probabilistic-interpretation-of-regularization/</guid><pubDate>Mon, 29 Aug 2016 12:52:33 GMT</pubDate></item><item><title>A Probabilistic View of Linear Regression</title><link>http://satisficing.briankeng.com/posts/a-probabilistic-view-of-regression/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;One thing that I always disliked about introductory material to linear
regression is how randomness is explained.  The explanations always
seemed unintuitive because, as I have frequently seen it, they appear as an
after thought rather than the central focus of the model.
In this post, I'm going to try to
take another approach to building an ordinary linear regression model starting
from a probabilistic point of view (which is pretty much just a Bayesian view).
After the general idea is established, I'll modify the model a bit and end up
with a Poisson regression using the exact same principles showing how
generalized linear models aren't any more complicated.  Hopefully, this will
help explain the "randomness" in linear regression in a more intuitive way.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/a-probabilistic-view-of-regression/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Bayesian</category><category>logistic</category><category>mathjax</category><category>Poisson</category><category>probability</category><category>regression</category><guid>http://satisficing.briankeng.com/posts/a-probabilistic-view-of-regression/</guid><pubDate>Sun, 15 May 2016 00:43:05 GMT</pubDate></item><item><title>Elementary Statistics for Direct Marketing</title><link>http://satisficing.briankeng.com/posts/normal-difference-distribution/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to look at some elementary statistics for direct marketing.
Most of the techniques are direct applications of topics learned in a first
year statistics course hence the "elementary".  I'll start off by covering some
background and terminology on the direct marketing and then introduce some of
the statistical inference techniques that are commonly used.  As usual, I'll
mix in some theory where appropriate to build some intuition.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/normal-difference-distribution/"&gt;Read more…&lt;/a&gt; (20 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>direct marketing</category><category>mathjax</category><category>normal</category><category>probability</category><category>sample size</category><guid>http://satisficing.briankeng.com/posts/normal-difference-distribution/</guid><pubDate>Sun, 28 Feb 2016 01:40:41 GMT</pubDate></item><item><title>A Primer on Statistical Inference and Hypothesis Testing</title><link>http://satisficing.briankeng.com/posts/hypothesis-testing/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is about some fundamental concepts in classical (or frequentist)
statistics: inference and hypothesis testing.  A while back, I came to the
realization that I didn't have a good intuition of these concepts (at least
not to my liking) beyond the mechanical nature of applying them.
What was missing was how they related to a probabilistic view of the subject.
This bothered me since having a good intuition about a subject is
probably the most useful (and fun!) part of learning a subject.  So this post
is a result of my re-education on these topics.  Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/hypothesis-testing/"&gt;Read more…&lt;/a&gt; (19 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frequentist statistics</category><category>hypothesis testing</category><category>mathjax</category><category>models</category><category>p-values</category><category>statistical inference</category><guid>http://satisficing.briankeng.com/posts/hypothesis-testing/</guid><pubDate>Sat, 09 Jan 2016 16:22:26 GMT</pubDate></item><item><title>Optimal Betting Strategies and The Kelly Criterion</title><link>http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;My last post was about some &lt;a class="reference external" href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers"&gt;common mistakes&lt;/a&gt; when betting
or gambling, even with a basic understanding of probability.  This post is going to
talk about the other side: optimal betting strategies using some very
interesting results from some very famous mathematicians in the 50s and 60s.
I'll spend a bit of time introducing some new concepts (at least to me), setting up the
problem and digging into some of the math.  We'll be looking at it from the
lens of our simplest probability problem: the coin flip.  A note: I will not be
covering the part that shows you how to make a fortune -- that's an exercise
best left to the reader.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>betting</category><category>Kelly Criterion</category><category>mathjax</category><category>probability</category><category>Shannon</category><category>Thorp</category><guid>http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/</guid><pubDate>Sun, 15 Nov 2015 21:13:31 GMT</pubDate></item><item><title>Probability as Extended Logic</title><link>http://satisficing.briankeng.com/posts/probability-the-logic-of-science/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;Modern probability theory is typically derived from the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Probability_axioms"&gt;Kolmogorov axioms&lt;/a&gt;,
using measure theory with concepts like events and sample space.
In one way, it's intuitive to understand how this works as Laplace
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Classical_definition_of_probability"&gt;wrote&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
The probability of an event is the ratio of the number of cases favorable
to it, to the number of all cases possible, when [the cases are] equally
possible. ... Probability is thus simply a fraction whose numerator is the
number of favorable cases and whose denominator is the number of all the
cases possible.&lt;/blockquote&gt;
&lt;p&gt;However, the intuition of this view of probability breaks down when we want to
do more complex reasoning.  After learning probability from the lens of coins,
dice and urns full of red and white balls, I still didn't feel that I had
have a strong grasp about how to apply it to other situations -- especially
ones where it was difficult or too abstract to apply the idea of &lt;em&gt;"a fraction
whose numerator is the number of favorable cases and whose denominator is the
number of all the cases possible"&lt;/em&gt;.  And then I read &lt;a class="reference external" href="http://www.cambridge.org/gb/academic/subjects/physics/theoretical-physics-and-mathematical-physics/probability-theory-logic-science"&gt;Probability Theory: The Logic of Science&lt;/a&gt; by E. T. Jayne.&lt;/p&gt;
&lt;p&gt;Jayne takes a drastically different approach to probability, not with events and
sample spaces, but rather as an extension of Boolean logic.  Taking this view made
a great deal of sense to me since I spent a lot of time &lt;a class="reference external" href="http://satisficing.briankeng.com/posts/accessible-satisfiability"&gt;studying and reasoning&lt;/a&gt; in Boolean logic.  The following post
is my attempt to explain Jayne's view of probability theory, where he derives
it from "common sense" extensions to Boolean logic.  (&lt;em&gt;Spoiler alert: he ends
up with pretty much the same mathematical system as Kolmogorov's probability
theory.&lt;/em&gt;) I'll stay away from any heavy derivations and stick with the
intuition, which is exactly where I think this view of probability theory is most
useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/probability-the-logic-of-science/"&gt;Read more…&lt;/a&gt; (14 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Jayne</category><category>logic</category><category>mathjax</category><category>probability</category><guid>http://satisficing.briankeng.com/posts/probability-the-logic-of-science/</guid><pubDate>Thu, 15 Oct 2015 00:30:05 GMT</pubDate></item><item><title>Accessible Satisfiability</title><link>http://satisficing.briankeng.com/posts/accessible-satisfiability/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;Even though it was just a scant few years ago, my research in grad school seems
like it was from another lifetime.  Nowadays I deal with data and most of my
code revolves around manipulating and extracting interesting insights from it.
However in my previous life I spent quite a bit of time dealing with
satisfiability problems.  So before I start writing about data and related
topics, I thought I'd kick it old school and write about some topics from my
formative years.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/accessible-satisfiability/"&gt;Read more…&lt;/a&gt; (17 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Boolean</category><category>formal</category><category>mathjax</category><category>NP-Complete</category><category>SAT</category><category>verification</category><guid>http://satisficing.briankeng.com/posts/accessible-satisfiability/</guid><pubDate>Sun, 04 Oct 2015 00:28:04 GMT</pubDate></item></channel></rss>