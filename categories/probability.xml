<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Bounded Rationality (probability)</title><link>http://satisficing.briankeng.com/</link><description></description><atom:link rel="self" type="application/rss+xml" href="http://satisficing.briankeng.com/categories/probability.xml"></atom:link><language>en</language><lastBuildDate>Sun, 13 Dec 2015 21:06:01 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Markov Chain Monte Carlo Methods, Rejection Sampling and the Metropolis-Hastings Algorithm</title><link>http://satisficing.briankeng.com/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post, I'm going to continue on the same theme from the last post: &lt;a href="http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution"&gt;random sampling&lt;/a&gt;.  We're going to look at two methods for sampling a distribution: rejection sampling and Markov Chain Monte Carlo Methods (MCMC) using the Metropolis Hastings algorithm.  As usual, I'll be providing a mix of intuitive explanations, theory and some examples with code.  Hopefully, this will help explain a relatively straight-forward topic that is frequently presented in a complex way.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/"&gt;Read more…&lt;/a&gt; (20 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Markov Chain</category><category>MCMC</category><category>Metropolis-Hastings</category><category>Monte Carlo</category><category>probability</category><category>rejection sampling</category><category>sampling</category><guid>http://satisficing.briankeng.com/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/</guid><pubDate>Sun, 13 Dec 2015 20:05:56 GMT</pubDate></item><item><title>Sampling from a Normal Distribution</title><link>http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;One of the most common probability distributions is the normal (or Gaussian) distribution.  Many natural phenomena can be modeled using a normal distribution.  It's also of great importance due to its relation to the &lt;a href="https://en.wikipedia.org/wiki/Central_limit_theorem"&gt;Central Limit Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we'll be reviewing the normal distribution and looking at how to draw samples from it using two methods.  The first method using the central limit theorem, and the second method using the &lt;a href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform"&gt;Box-Muller transform&lt;/a&gt;.  As usual, some brief coverage of the mathematics and code will be included to help drive intuition.
&lt;/p&gt;&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>normal distribution</category><category>probability</category><category>sampling</category><guid>http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution/</guid><pubDate>Sun, 29 Nov 2015 02:57:02 GMT</pubDate></item><item><title>Optimal Betting Strategies and The Kelly Criterion</title><link>http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;My last post was about some &lt;a class="reference external" href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers"&gt;common mistakes&lt;/a&gt; when betting
or gambling, even with a basic understanding of probability.  This post is going to
talk about the other side: optimal betting strategies using some very
interesting results from some very famous mathematicians in the 50s and 60s.
I'll spend a bit of time introducing some new concepts (at least to me), setting up the
problem and digging into some of the math.  We'll be looking at it from the
lens of our simplest probability problem: the coin flip.  A note: I will not be
covering the part that shows you how to make a fortune -- that's an exercise
best left to the reader.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>betting</category><category>Kelly Criterion</category><category>mathjax</category><category>probability</category><category>Shannon</category><category>Thorp</category><guid>http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/</guid><pubDate>Sun, 15 Nov 2015 21:13:31 GMT</pubDate></item><item><title>The Gambler's Fallacy and The Law of Small Numbers</title><link>http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Games and gambling have been part of human cultures around the world for millennia.  Nowadays, the connection between games of chance and mathematics (in particular probability) are so obvious that it is taught to school children.  However, the mathematics of games and gambling only started to formally &lt;a href="https://en.wikipedia.org/wiki/Gambling#History"&gt;develop&lt;/a&gt; in the 17th century with the works of multiple mathematicians such as Fermat and Pascal.  It is then no wonder that many incorrect beliefs around gambling have formed that are "intuitive" from a layman's perspective but fail to pass muster when applying the rigor of mathematics.&lt;/p&gt;
&lt;p&gt;In this post, I want to discuss how surprisingly easy it is to be fooled into the wrong line of thinking even when approaching it using mathematics.  We'll take a look from both a theoretical (mathematics) point of view looking at topics such as the &lt;a href="https://en.wikipedia.org/wiki/Gambler's_fallacy"&gt;Gambler's Fallacy&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Hasty_generalization"&gt;law of small numbers&lt;/a&gt; as well as do some simulations using code to gain some insight into the problem.  This post was inspired by a paper I recently came across a paper by Miller and Sanjurjo&lt;a href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/#fn-1"&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; that explains the surprising result of how easily we can be fooled.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Gambler's Fallacy</category><category>Law of Large Numbers</category><category>Law of Small Numbers</category><category>probability</category><guid>http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/</guid><pubDate>Sun, 01 Nov 2015 15:08:11 GMT</pubDate></item><item><title>Probability as Extended Logic</title><link>http://satisficing.briankeng.com/posts/probability-the-logic-of-science/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;Modern probability theory is typically derived from the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Probability_axioms"&gt;Kolmogorov axioms&lt;/a&gt;,
using measure theory with concepts like events and sample space.
In one way, it's intuitive to understand how this works as Laplace
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Classical_definition_of_probability"&gt;wrote&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
The probability of an event is the ratio of the number of cases favorable
to it, to the number of all cases possible, when [the cases are] equally
possible. ... Probability is thus simply a fraction whose numerator is the
number of favorable cases and whose denominator is the number of all the
cases possible.&lt;/blockquote&gt;
&lt;p&gt;However, the intuition of this view of probability breaks down when we want to
do more complex reasoning.  After learning probability from the lens of coins,
dice and urns full of red and white balls, I still didn't feel that I had
have a strong grasp about how to apply it to other situations -- especially
ones where it was difficult or too abstract to apply the idea of &lt;em&gt;"a fraction
whose numerator is the number of favorable cases and whose denominator is the
number of all the cases possible"&lt;/em&gt;.  And then I read &lt;a class="reference external" href="http://www.cambridge.org/gb/academic/subjects/physics/theoretical-physics-and-mathematical-physics/probability-theory-logic-science"&gt;Probability Theory: The Logic of Science&lt;/a&gt; by E. T. Jayne.&lt;/p&gt;
&lt;p&gt;Jayne takes a drastically different approach to probability, not with events and
sample spaces, but rather as an extension of Boolean logic.  Taking this view made
a great deal of sense to me since I spent a lot of time &lt;a class="reference external" href="http://satisficing.briankeng.com/posts/accessible-satisfiability"&gt;studying and reasoning&lt;/a&gt; in Boolean logic.  The following post
is my attempt to explain Jayne's view of probability theory, where he derives
it from "common sense" extensions to Boolean logic.  (&lt;em&gt;Spoiler alert: he ends
up with pretty much the same mathematical system as Kolmogorov's probability
theory.&lt;/em&gt;) I'll stay away from any heavy derivations and stick with the
intuition, which is exactly where I think this view of probability theory is most
useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/probability-the-logic-of-science/"&gt;Read more…&lt;/a&gt; (14 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Jayne</category><category>logic</category><category>mathjax</category><category>probability</category><guid>http://satisficing.briankeng.com/posts/probability-the-logic-of-science/</guid><pubDate>Thu, 15 Oct 2015 00:30:05 GMT</pubDate></item></channel></rss>