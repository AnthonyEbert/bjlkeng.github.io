<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Bounded Rationality</title><link>http://satisficing.briankeng.com/</link><description>Understanding programming, data, and math to a satisfactory degree.</description><atom:link href="http://satisficing.briankeng.com/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 11 Jun 2016 22:13:03 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Beyond Collaborative Filtering</title><link>http://satisficing.briankeng.com/posts/beyond-collaborative-filtering/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;I wrote a couple of posts about some of the work on recommendation systems and
collaborative filtering that we're doing at my job as a Data Scientist at
&lt;a class="reference external" href="http://www.rubikloud.com"&gt;Rubikloud&lt;/a&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://rubikloud.com/labs/data-science/beyond-collaborative-filtering/"&gt;Beyond Collaborative Filtering (Part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://rubikloud.com/labs/data-science/beyond-collaborative-filtering-part-2/"&gt;Beyond Collaborative Filtering (Part 2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's a blurb:&lt;/p&gt;
&lt;blockquote&gt;
Here at Rubikloud, a big focus of our data science team is empowering retailers
in delivering personalized one-to-one communications with their customers. A
big aspect of personalization is recommending products and services that are
tailored to a customer’s wants and needs. Naturally, recommendation systems are
an active research area in machine learning with practical large scale
deployments from companies such as Netflix and Spotify. In Part 1 of this
series, I’ll describe the unique challenges that we have faced in building a
retail specific product recommendation system and outline one of the main
components of our recommendation system: a collaborative filtering algorithm.
In Part 2, I’ll follow up with several useful applications of collaborative
filtering and end by highlighting some of its limitations.&lt;/blockquote&gt;
&lt;p&gt;Hope you like it!&lt;/p&gt;&lt;/div&gt;</description><category>Collaborative Filtering</category><category>Machine Learning</category><category>Recommendation Systems</category><guid>http://satisficing.briankeng.com/posts/beyond-collaborative-filtering/</guid><pubDate>Sat, 11 Jun 2016 22:00:34 GMT</pubDate></item><item><title>A Probabilistic View of Linear Regression</title><link>http://satisficing.briankeng.com/posts/a-probabilistic-view-of-regression/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;One thing that I always disliked about introductory material to linear
regression is how randomness is explained.  The explanations always
seemed unintuitive because, as I have frequently seen it, they appear as an
after thought rather than the central focus of the model.
In this post, I'm going to try to
take another approach to building an ordinary linear regression model starting
from a probabilistic point of view (which is pretty much just a Bayesian view).
After the general idea is established, I'll modify the model a bit and end up
with a Poisson regression using the exact same principles showing how
generalized linear models aren't any more complicated.  Hopefully, this will
help explain the "randomness" in linear regression in a more intuitive way.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/a-probabilistic-view-of-regression/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Bayesian</category><category>logistic</category><category>mathjax</category><category>Poisson</category><category>probability</category><category>regression</category><guid>http://satisficing.briankeng.com/posts/a-probabilistic-view-of-regression/</guid><pubDate>Sun, 15 May 2016 00:43:05 GMT</pubDate></item><item><title>Normal Approximation to the Posterior Distribution</title><link>http://satisficing.briankeng.com/posts/normal-approximations-to-the-posterior-distribution/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post, I'm going to write about how the ever versatile normal distribution can be used to approximate a Bayesian posterior distribution.  Unlike some other normal approximations, this is &lt;em&gt;not&lt;/em&gt; a direct application of the central limit theorem.  The result has a straight forward proof using Laplace's Method whose main ideas I will attempt to present.  I'll also simulate a simple scenario to see how it works in practice.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/normal-approximations-to-the-posterior-distribution/"&gt;Read more…&lt;/a&gt; (14 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Bayesian</category><category>normal distribution</category><category>posterior</category><category>prior</category><category>probability</category><category>sampling</category><guid>http://satisficing.briankeng.com/posts/normal-approximations-to-the-posterior-distribution/</guid><pubDate>Sat, 02 Apr 2016 19:22:54 GMT</pubDate></item><item><title>The Empirical Distribution Function</title><link>http://satisficing.briankeng.com/posts/the-empirical-distribution-function/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This post is going to look at a useful non-parametric method for estimating the cumulative distribution function (CDF) of a random variable called the &lt;a href="https://en.wikipedia.org/wiki/Empirical_distribution_function"&gt;empirical distribution function&lt;/a&gt; (sometimes called the empirical CDF).  We'll talk a bit about the mechanics of computing it, some theory about its confidence intervals and also do some simulations to gain some intuition about how it behaves.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/the-empirical-distribution-function/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CDF</category><category>confidence bands</category><category>confidence intervals</category><category>empirical distribution function</category><guid>http://satisficing.briankeng.com/posts/the-empirical-distribution-function/</guid><pubDate>Sun, 13 Mar 2016 01:08:21 GMT</pubDate></item><item><title>Elementary Statistics for Direct Marketing</title><link>http://satisficing.briankeng.com/posts/normal-difference-distribution/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to look at some elementary statistics for direct marketing.
Most of the techniques are direct applications of topics learned in a first
year statistics course hence the "elementary".  I'll start off by covering some
background and terminology on the direct marketing and then introduce some of
the statistical inference techniques that are commonly used.  As usual, I'll
mix in some theory where appropriate to build some intuition.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/normal-difference-distribution/"&gt;Read more…&lt;/a&gt; (20 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>direct marketing</category><category>mathjax</category><category>normal</category><category>probability</category><category>sample size</category><guid>http://satisficing.briankeng.com/posts/normal-difference-distribution/</guid><pubDate>Sun, 28 Feb 2016 01:40:41 GMT</pubDate></item><item><title>A Primer on Statistical Inference and Hypothesis Testing</title><link>http://satisficing.briankeng.com/posts/hypothesis-testing/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is about some fundamental concepts in classical (or frequentist)
statistics: inference and hypothesis testing.  A while back, I came to the
realization that I didn't have a good intuition of these concepts (at least
not to my liking) beyond the mechanical nature of applying them.
What was missing was how they related to a probabilistic view of the subject.
This bothered me since having a good intuition about a subject is
probably the most useful (and fun!) part of learning a subject.  So this post
is a result of my re-education on these topics.  Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/hypothesis-testing/"&gt;Read more…&lt;/a&gt; (19 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>frequentist statistics</category><category>hypothesis testing</category><category>mathjax</category><category>models</category><category>p-values</category><category>statistical inference</category><guid>http://satisficing.briankeng.com/posts/hypothesis-testing/</guid><pubDate>Sat, 09 Jan 2016 16:22:26 GMT</pubDate></item><item><title>Markov Chain Monte Carlo Methods, Rejection Sampling and the Metropolis-Hastings Algorithm</title><link>http://satisficing.briankeng.com/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post, I'm going to continue on the same theme from the last post: &lt;a href="http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution"&gt;random sampling&lt;/a&gt;.  We're going to look at two methods for sampling a distribution: rejection sampling and Markov Chain Monte Carlo Methods (MCMC) using the Metropolis Hastings algorithm.  As usual, I'll be providing a mix of intuitive explanations, theory and some examples with code.  Hopefully, this will help explain a relatively straight-forward topic that is frequently presented in a complex way.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/"&gt;Read more…&lt;/a&gt; (20 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Markov Chain</category><category>MCMC</category><category>Metropolis-Hastings</category><category>Monte Carlo</category><category>probability</category><category>rejection sampling</category><category>sampling</category><guid>http://satisficing.briankeng.com/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/</guid><pubDate>Sun, 13 Dec 2015 20:05:56 GMT</pubDate></item><item><title>Sampling from a Normal Distribution</title><link>http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;One of the most common probability distributions is the normal (or Gaussian) distribution.  Many natural phenomena can be modeled using a normal distribution.  It's also of great importance due to its relation to the &lt;a href="https://en.wikipedia.org/wiki/Central_limit_theorem"&gt;Central Limit Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we'll be reviewing the normal distribution and looking at how to draw samples from it using two methods.  The first method using the central limit theorem, and the second method using the &lt;a href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform"&gt;Box-Muller transform&lt;/a&gt;.  As usual, some brief coverage of the mathematics and code will be included to help drive intuition.
&lt;/p&gt;&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution/"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>normal distribution</category><category>probability</category><category>sampling</category><guid>http://satisficing.briankeng.com/posts/sampling-from-a-normal-distribution/</guid><pubDate>Sun, 29 Nov 2015 02:57:02 GMT</pubDate></item><item><title>Optimal Betting Strategies and The Kelly Criterion</title><link>http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;My last post was about some &lt;a class="reference external" href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers"&gt;common mistakes&lt;/a&gt; when betting
or gambling, even with a basic understanding of probability.  This post is going to
talk about the other side: optimal betting strategies using some very
interesting results from some very famous mathematicians in the 50s and 60s.
I'll spend a bit of time introducing some new concepts (at least to me), setting up the
problem and digging into some of the math.  We'll be looking at it from the
lens of our simplest probability problem: the coin flip.  A note: I will not be
covering the part that shows you how to make a fortune -- that's an exercise
best left to the reader.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>betting</category><category>Kelly Criterion</category><category>mathjax</category><category>probability</category><category>Shannon</category><category>Thorp</category><guid>http://satisficing.briankeng.com/posts/optimal-betting-and-the-kelly-criterion/</guid><pubDate>Sun, 15 Nov 2015 21:13:31 GMT</pubDate></item><item><title>The Gambler's Fallacy and The Law of Small Numbers</title><link>http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Games and gambling have been part of human cultures around the world for millennia.  Nowadays, the connection between games of chance and mathematics (in particular probability) are so obvious that it is taught to school children.  However, the mathematics of games and gambling only started to formally &lt;a href="https://en.wikipedia.org/wiki/Gambling#History"&gt;develop&lt;/a&gt; in the 17th century with the works of multiple mathematicians such as Fermat and Pascal.  It is then no wonder that many incorrect beliefs around gambling have formed that are "intuitive" from a layman's perspective but fail to pass muster when applying the rigor of mathematics.&lt;/p&gt;
&lt;p&gt;In this post, I want to discuss how surprisingly easy it is to be fooled into the wrong line of thinking even when approaching it using mathematics.  We'll take a look from both a theoretical (mathematics) point of view looking at topics such as the &lt;a href="https://en.wikipedia.org/wiki/Gambler's_fallacy"&gt;Gambler's Fallacy&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Hasty_generalization"&gt;law of small numbers&lt;/a&gt; as well as do some simulations using code to gain some insight into the problem.  This post was inspired by a paper I recently came across a paper by Miller and Sanjurjo&lt;a href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/#fn-1"&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; that explains the surprising result of how easily we can be fooled.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Gambler's Fallacy</category><category>Law of Large Numbers</category><category>Law of Small Numbers</category><category>probability</category><guid>http://satisficing.briankeng.com/posts/gamblers-fallacy-and-the-law-of-small-numbers/</guid><pubDate>Sun, 01 Nov 2015 15:08:11 GMT</pubDate></item></channel></rss>